{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 1000 training Dataset with three features using pandas in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.523030</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-0.234137</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "      <td>-0.469474</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.542560</td>\n",
       "      <td>-0.463418</td>\n",
       "      <td>-0.465730</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.241962</td>\n",
       "      <td>-1.913280</td>\n",
       "      <td>-1.724918</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.960046</td>\n",
       "      <td>-0.122709</td>\n",
       "      <td>0.093372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.130204</td>\n",
       "      <td>2.411677</td>\n",
       "      <td>1.516394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.602118</td>\n",
       "      <td>0.072037</td>\n",
       "      <td>-0.212209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.951918</td>\n",
       "      <td>0.077481</td>\n",
       "      <td>0.257753</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.241761</td>\n",
       "      <td>0.334176</td>\n",
       "      <td>-0.155259</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3    y\n",
       "0    0.496714 -0.138264  0.647689  1.0\n",
       "1    1.523030 -0.234153 -0.234137  1.0\n",
       "2    1.579213  0.767435 -0.469474  1.0\n",
       "3    0.542560 -0.463418 -0.465730  1.0\n",
       "4    0.241962 -1.913280 -1.724918  1.0\n",
       "..        ...       ...       ...  ...\n",
       "995 -0.960046 -0.122709  0.093372  0.0\n",
       "996 -1.130204  2.411677  1.516394  0.0\n",
       "997  0.602118  0.072037 -0.212209  0.0\n",
       "998 -0.951918  0.077481  0.257753  0.0\n",
       "999 -1.241761  0.334176 -0.155259  0.0\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "training = pd.DataFrame(np.random.randn(1000,3),columns = ['x1','x2','x3'])\n",
    "\n",
    "training['y'] = np.ones(1000)\n",
    "midpoints = int(training.shape[0]/2)\n",
    "training['y'][midpoints:] = 0\n",
    "trainingAfterShuffle = training.sample(frac = 1)\n",
    "training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building two layer neural network object..\n",
    "1 hidden layer and 1 output layer\n",
    "\n",
    "hidden layer size = 4\n",
    "output layer size = 1\n",
    "\n",
    "loss function = SSE\n",
    "\n",
    "training sample size = (1000X3)\n",
    "\n",
    "hidden layer input weight matrix (3X4) variable name = pre_active_weights\n",
    "\n",
    "hidden layer output weight matrix (4X1) variable name = activated_weights\n",
    "\n",
    "hidden layer and output layers activation function = sigmoid\n",
    "\n",
    "loss function = SSE()\n",
    "\n",
    "dot product between two matrices function name = linearDot()\n",
    "\n",
    "sigmoid derivative function = sigder()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Granite\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221fef85a48>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dimensions(x):\n",
    "    for i in x:\n",
    "        print(i.shape)\n",
    "\n",
    "\n",
    "\n",
    "class neural_network:\n",
    "    def __init__(self,feature_size,inputs,hidden_layer_size,output_layer_size,loss_function):\n",
    "        self.feature_size = feature_size\n",
    "        self.inputs = inputs   #training set\n",
    "        self.y = inputs[:,self.feature_size]\n",
    "        self.y = np.reshape(self.y,(1000,1))\n",
    "        self.output_layers = output_layer_size\n",
    "        self.loss_func = loss_function\n",
    "        self.y_pred = []\n",
    "        self.inputs = np.delete(self.inputs,self.feature_size,1)\n",
    "        instances = self.inputs.shape[0]\n",
    "        self.pre_active_weights = np.random.randn(self.feature_size,hidden_layer_size)\n",
    "        self.activated_weights = np.random.randn(hidden_layer_size,self.output_layers)\n",
    "        self.preActive = []\n",
    "        self.trainerrors = []\n",
    "        self.testerrors = []\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        val = 1/(1+np.exp(-X))\n",
    "        return val*(1-val)\n",
    "\n",
    "    def sigder(self,X):\n",
    "        return X*(1-X)\n",
    "    \n",
    "    def linearDot(self,X,W):\n",
    "        return np.dot(X,W)\n",
    "    \n",
    "    \n",
    "    def SSE(self,y,y_pred):\n",
    "        errors = (y-y_pred)**2\n",
    "        return np.sum(errors)\n",
    "    \n",
    "    def softmax(X):\n",
    "        shiftx = X - np.max(X)\n",
    "        exps = np.exp(shiftx)\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def crossEntropyLoss(output,label):\n",
    "        return output-label\n",
    "    \n",
    "    def trainForSoftMax(self,traindata,lr,epoch,hot_label):\n",
    "#           --------------forward propagation-----------------\n",
    "            self.preActive = self.linearDot(self.inputs, self.pre_active_weights)\n",
    "            sigmoided = self.sigmoid(self.preActive)\n",
    "            self.y_pred = self.linearDot(sigmoided,self.activated_weights)\n",
    "            self.y_pred_sigmoid = self.sigmoid(self.y_pred)\n",
    "            softmax = softmax(self.y_pred_sigmoid)\n",
    "            dloss_da = softmax-hot_label\n",
    "             \n",
    "    \n",
    "    \n",
    "    def train(self,lr,epoch):\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "#           --------------forward propagation-----------------\n",
    "            self.preActive = self.linearDot(self.inputs, self.pre_active_weights)\n",
    "            sigmoided = self.sigmoid(self.preActive)\n",
    "            self.y_pred = self.linearDot(sigmoided,self.activated_weights)\n",
    "            self.y_pred_sigmoid = self.sigmoid(self.y_pred)\n",
    "\n",
    "#           --------------error calculation----------------\n",
    "            \n",
    "            errors = self.SSE(self.y,self.y_pred_sigmoid)\n",
    "\n",
    "#           --------------gradient calculation and backpropagation----------\n",
    "            derrors_dyps = -2*np.sum(self.y-self.y_pred_sigmoid)\n",
    "            derrors_dyps = np.reshape(derrors_dyps,(1,1))\n",
    "            dyps_dyp = self.sigder(self.y_pred_sigmoid)\n",
    "            dyp_dw_active = sigmoided\n",
    "        \n",
    "            dyp_ds = self.activated_weights\n",
    "            ds_dz = self.sigder(sigmoided)\n",
    "            dz_dw_pre = self.inputs.T\n",
    "            derrors_dyp = np.dot(dyps_dyp,derrors_dyps)\n",
    "            derrors_dw_act =np.dot(sigmoided.T,derrors_dyp)\n",
    "            derrors_dw_pre = np.dot(dz_dw_pre,np.dot(derrors_dyp,dyp_ds.T))\n",
    "        \n",
    "#           updating weights\n",
    "            self.activated_weights -= lr*derrors_dw_act\n",
    "            self.pre_active_weights -= lr*derrors_dw_pre\n",
    "\n",
    "#           storing error values for plotting\n",
    "            self.trainerrors.append(errors)\n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def test(self,testData,y):\n",
    "        pre = self.linearDot(testData, self.pre_active_weights)\n",
    "        sigmoided = self.sigmoid(pre)\n",
    "        y_pred = self.linearDot(sigmoided,self.activated_weights)\n",
    "        \n",
    "        y_pred_sigmoid = self.sigmoid(y_pred)\n",
    "        \n",
    "        errors = self.SSE(y,y_pred_sigmoid)\n",
    "        self.testerrors.append(errors)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "inputs = training.to_numpy()          \n",
    "nn = neural_network(feature_size = 3,inputs = inputs,\n",
    "                    hidden_layer_size = 4,output_layer_size = 1,\n",
    "                    loss_function = 'SSE')\n",
    "epoch = 20000\n",
    "nn.train(0.0003,epoch)\n",
    "iteration = [i for i in range(epoch)]\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel(\"errors\")\n",
    "plt.plot(iteration,nn.trainerrors)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating test dateset....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.DataFrame(np.random.randn(100,3),columns = ['x1','x2','x3'])\n",
    "\n",
    "testing['y'] = np.ones(100)\n",
    "midpoints = int(testing.shape[0]/2)\n",
    "testing['y'][midpoints:] = 0\n",
    "testing = testing.sample(frac = 1)\n",
    "label = testing['y'].to_numpy()\n",
    "label = np.reshape(label,(100,1))\n",
    "testing = np.delete(testing.to_numpy(),testing.shape[1]-1,1)\n",
    "\n",
    "# testing  = np.delete(testing)\n",
    "# # testing = np.delete(testing,4,1)\n",
    "# # testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward propagating the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testloss using SSE:  31.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Granite\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "nn.test(testing,label)\n",
    "for x in nn.testerrors:\n",
    "    print(\"testloss using SSE: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the training dataset for classification using softmax  cross entropy loss.\n",
    "We are gonna use one hot label encoding to classify two different classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training = training.drop('y',axis = 1)\n",
    "training\n",
    "x1 = np.tile(np.array([1,0]),(500,1))\n",
    "x2 = np.tile(np.array([0,1]),(500,1))\n",
    "labels = np.vstack((x1,x2))\n",
    "labels.shape\n",
    "\n",
    "training= training.to_numpy()\n",
    "trainingset = np.append(training,labels,axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = np.append(training,labels,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(trainingset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotencoding = np.array(list(zip(trainingset[:,4],trainingset[:,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
