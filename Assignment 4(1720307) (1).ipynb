{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258c7aa91bf847a9a07f656e33e02db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8387c86073b84d51920fba18901d8f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57730072f124fd89e700de4f0bd1adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bbd6effd364c4d92c91488c2e18aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets , transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n",
    "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 600, shuffle = True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e7a168cf191d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "\n",
    "image,label = dataiter.next()\n",
    "print(image.shape)\n",
    "\n",
    "plt.imshow(image[0].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Model for CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class camnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(camnet,self).__init__()\n",
    "        \n",
    "        self.convlayers = nn.Sequential(\n",
    "            nn.Conv2d(1,6,kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(inplace=True), #26x26\n",
    "            nn.MaxPool2d(kernel_size=(2,2),stride=2), #13x13\n",
    "            nn.Conv2d(6,16,kernel_size=(2,2)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),#12x12\n",
    "            nn.MaxPool2d(kernel_size=(2,2),stride=2),#6x6\n",
    "            nn.Conv2d(16,120,kernel_size=(2,2)),\n",
    "            nn.BatchNorm2d(120),\n",
    "            nn.ReLU(inplace=True),#5x5\n",
    "            nn.Conv2d(120,240,kernel_size=(2,2)), #4x4\n",
    "            nn.BatchNorm2d(240))\n",
    "        \n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=(4,4))) # pooling average value from each channel\n",
    "\n",
    "        self.gap_layer = nn.Sequential(\n",
    "            nn.Linear(240,10),\n",
    "            nn.Softmax(dim=-1))\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = self.convlayers(x)\n",
    "        output = self.last_layer(output)\n",
    "        output = torch.squeeze(output)\n",
    "        output = self.gap_layer(output)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "cam = camnet()\n",
    "cam.cuda()\n",
    "cam_criterion = nn.CrossEntropyLoss()\n",
    "cam_optimizer = optim.SGD(cam.parameters(),lr = 0.1 , momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.load_state_dict(torch.load('C:\\\\Users\\\\Granite\\\\Desktop\\\\camnet.pth'))\n",
    "\n",
    "loss_per_epoch = []\n",
    "def train(epoch):\n",
    "    cam.train()\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        running_loss = 0\n",
    "        avg_loss = 0\n",
    "        for j , batch in enumerate(trainloader):\n",
    "            \n",
    "            \n",
    "            image,label = batch\n",
    "            \n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            \n",
    "            \n",
    "            output =cam(image)\n",
    "            \n",
    "            cam_optimizer.zero_grad()\n",
    "            loss = cam_criterion(output,label)\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            cam_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.detach()\n",
    "            \n",
    "            avg_loss = running_loss/100\n",
    "        \n",
    "        print('average loss '+str(avg_loss.item())+' on '+str(i)+' epoch') \n",
    "        loss_per_epoch.append(avg_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(10) #actually i ran it 800 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cam.state_dict(),'C:\\\\Users\\\\Granite\\\\Desktop\\\\camnet.pth') #the whole parameter model is saved and given with the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample = nn.Upsample(size=(28,28),mode='bilinear') #upsampling method\n",
    "relu = nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for creating CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cam(feature_map,net,index):\n",
    "    matrix = net.gap_layer[0].weight\n",
    "    relued = relu(feature_map)\n",
    "    relued = torch.squeeze(relued)\n",
    "    relued_flattened = relued.view(-1,16)\n",
    "    pulled_vector = matrix[index,:]\n",
    "    #print(relued_flattened)\n",
    "    for i in range(240):\n",
    "        relued_flattened[i,:] = pulled_vector[i]*relued_flattened[i,:]\n",
    "    reshaped = torch.reshape(relued_flattened,(240,4,4))\n",
    "    output = upsample(torch.unsqueeze(torch.unsqueeze(torch.sum(reshaped,0),0),0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    cam.eval()\n",
    "    correct,total = 0,0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images,labels = data\n",
    "           \n",
    "            images = images.cuda()\n",
    "            \n",
    "            #print('shape of image', images.shape)\n",
    "            labels = labels.cuda()\n",
    "            f_map = cam.convlayers(images)\n",
    "            l_layer = torch.squeeze(cam.last_layer(f_map))\n",
    "            print('l_layer shape',l_layer.shape)\n",
    "            outputs = cam.gap_layer(l_layer)\n",
    "            print('outputs shape',outputs.shape)\n",
    "      \n",
    "            print('feature_map shape',f_map.shape)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "        \n",
    "            _,predicted = torch.max(outputs.data,0)\n",
    "            total += 1\n",
    "            corrected = predicted==labels\n",
    "        \n",
    "            if corrected:\n",
    "            \n",
    "                cam_image = create_cam(f_map,cam,predicted).cpu().numpy()\n",
    "                cam_image = np.squeeze(cam_image)\n",
    "                plt.imshow(cam_image)\n",
    "                plt.pause(0.1)\n",
    "                plt.imshow(np.squeeze(images.cpu().numpy()))\n",
    "                plt.pause(0.1)\n",
    "                print('predicted for ',labels)\n",
    "                #break\n",
    "           \n",
    "      \n",
    "            \n",
    "            #print(labels,predicted,corrected)  \n",
    "            correct += (predicted==labels).sum().item()\n",
    "        accuracy = (correct/total)*100\n",
    "    \n",
    "        #print('correct on test set '+ str(correct))\n",
    "       # print('accuracy on test set '+ str(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of overlapping, class activation mapping is shown followed by the corresponding correctly classified image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.gap_layer[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
